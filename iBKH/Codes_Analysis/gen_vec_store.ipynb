{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_folder = 'Data/iBKH/'\n",
    "triplet_path = 'Data/triplets/'\n",
    "dict_path = 'Data/dicts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_folder = os.path.join(kg_folder, 'Entity')\n",
    "entity_files = glob.glob(os.path.join(entity_folder, '*_vocab.csv'))\n",
    "\n",
    "all_ents = pd.concat([pd.read_csv(file) for file in entity_files])\n",
    "all_ents.to_csv(os.path.join(entity_folder, 'all_ent.csv'), columns=['primary', 'name'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = []\n",
    "ids = []\n",
    "names = []\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "with open(kg_folder + \"Entity/all_ent.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for id, name in reader:\n",
    "        if id in id2label:\n",
    "            continue\n",
    "        # print(id, name)\n",
    "        doc = Document(page_content=name, metadata={\"id\":id})\n",
    "        docs.append(doc)\n",
    "        ids.append(id)\n",
    "        names.append(name)\n",
    "        id2label[id] = name\n",
    "        label2id[name] = id\n",
    "\n",
    "print(len(docs), docs[-1], ids[-1], names[-1], id2label[ids[-1]], label2id[names[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "id2labelfile = 'id2label.pkl'\n",
    "label2idfile = 'label2id.pkl'\n",
    "\n",
    "with open(kg_folder + \"Entity/\" + id2labelfile, 'wb') as file:\n",
    "    pickle.dump(id2label, file)\n",
    "\n",
    "with open(kg_folder + \"Entity/\" + label2idfile, 'wb') as file:\n",
    "    pickle.dump(label2id, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = pd.read_csv(triplet_path + 'triplet_whole.csv')\n",
    "head_list = list(triplet.drop_duplicates(subset='Head', keep='first')['Head'])\n",
    "tail_list = list(triplet.drop_duplicates(subset='Tail', keep='first')['Tail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(triplet['Relation'].unique())\n",
    "\n",
    "with open(kg_folder + \"Relation/\" + 'relations.json', 'w') as f:\n",
    "    json.dump(relations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head -> list of tails\n",
    "head2tails_dict = {}\n",
    "for head_entity in head_list:\n",
    "    sub_df = triplet.loc[triplet['Head'] == head_entity]\n",
    "    mapped_tail_list = list(sub_df.drop_duplicates(subset='Tail', keep='first')['Tail'])\n",
    "    head2tails_dict[head_entity] = mapped_tail_list\n",
    "\n",
    "with open(dict_path + 'head2tails_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(head2tails_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail -> list of heads\n",
    "tail2heads_dict = {}\n",
    "for tail_entity in tail_list:\n",
    "    sub_df = triplet.loc[triplet['Tail'] == tail_entity]\n",
    "    mapped_head_list = list(sub_df.drop_duplicates(subset='Head', keep='first')['Head'])\n",
    "    tail2heads_dict[tail_entity] = mapped_head_list\n",
    "\n",
    "with open(dict_path + 'tail2heads_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tail2heads_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head -> list of rels\n",
    "head2rels_dict = {}\n",
    "for head_entity in head_list:\n",
    "    sub_df = triplet.loc[triplet['Head'] == head_entity]\n",
    "    mapped_rel_list = list(sub_df.drop_duplicates(subset='Relation', keep='first')['Relation'])\n",
    "    head2rels_dict[head_entity] = mapped_rel_list\n",
    "\n",
    "with open(dict_path + 'head2rels_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(head2rels_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail -> list of rels\n",
    "tail2rels_dict = {}\n",
    "for tail_entity in tqdm(tail_list):\n",
    "    sub_df = triplet.loc[triplet['Tail'] == tail_entity]\n",
    "    mapped_rel_list = list(sub_df.drop_duplicates(subset='Relation', keep='first')['Relation'])\n",
    "    tail2rels_dict[tail_entity] = mapped_rel_list\n",
    "\n",
    "with open(dict_path + 'tail2rels_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tail2rels_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head@rel -> list of tails\n",
    "head_rel2tails_dict = {}\n",
    "for (head, relation), group in tqdm(triplet.groupby(['Head', 'Relation']), desc=\"Processing groups\"):\n",
    "    key = f'{head}@{relation}'\n",
    "    tail_list = list(group['Tail'].unique())\n",
    "    head_rel2tails_dict[key] = tail_list\n",
    "\n",
    "with open(dict_path + 'head@rel2tails_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(head_rel2tails_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail@rel -> list of heads\n",
    "tail_rel2heads_dict = {}\n",
    "for (tail, relation), group in tqdm(triplet.groupby(['Tail', 'Relation']), desc=\"Processing groups\"):\n",
    "    key = f'{tail}@{relation}'\n",
    "    head_list = list(group['Head'].unique())\n",
    "    tail_rel2heads_dict[key] = head_list\n",
    "\n",
    "with open(dict_path + 'tail@rel2heads_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tail_rel2heads_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel@tail -> list of heads\n",
    "rel_tail2heads_dict = {}\n",
    "for (tail, relation), group in tqdm(triplet.groupby(['Tail', 'Relation']), desc=\"Processing groups\"):\n",
    "    key = f'{relation}@{tail}'\n",
    "    head_list2 = list(group['Head'].unique())\n",
    "    rel_tail2heads_dict[key] = head_list2\n",
    "\n",
    "with open(dict_path + 'rel@tail2heads_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(rel_tail2heads_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_pkl(path=dict_path ,filename='tail2heads_dict.pkl'):\n",
    "    with open(path + filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for i, (key, value) in enumerate(data.items()):\n",
    "            print(f\"{key}: {value}\")\n",
    "            if i == 4:\n",
    "                break\n",
    "\n",
    "examine_pkl(filename='rel@tail2heads_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"ibkh_collection_cosine\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"./chroma_langchain_db_cosine\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(docs):\n",
    "    print(i)\n",
    "    vector_store.add_documents(documents=docs[i:i+5000])\n",
    "    i+=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = 'water-induced urticaria'\n",
    "results = vector_store.similarity_search_with_score(\n",
    "        te, k=4\n",
    "    )\n",
    "print(results)\n",
    "for doc, score in results:\n",
    "    print(doc.page_content, doc.metadata['id'], score)\n",
    "# res, score = results[0]\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = 'fever'\n",
    "results = vector_store.similarity_search_with_relevance_scores(\n",
    "        te, k=4\n",
    "    )\n",
    "print(results)\n",
    "for doc, score in results:\n",
    "    print(doc.page_content, doc.metadata['id'], score)\n",
    "# res, score = results[0]\n",
    "# print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibkh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
