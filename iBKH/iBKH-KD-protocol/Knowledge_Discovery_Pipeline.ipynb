{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iBKH-based Knowledge Discovery Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of Knowledge Discovery pipeline in our iBKH portal at http://ibkh.ai/.\n",
    "\n",
    "Given a target entity of interest, the task is to discover the Top-N entities from different entity types (currently supporting gene, drug, symptom, and pathway entities) that potentially link to the target entity. \n",
    "\n",
    "\n",
    "Generally, the pipeline contains 3 steps, including: \n",
    "1. Data preparation (triplets generation); \n",
    "\n",
    "2. Knowledge graph embedding learning; \n",
    "\n",
    "3. Knowledge discovery based on link prediction – predicting drug entities that potentially link to AD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 – Data preparation (triplets generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Collecting iBKH knowledge graph source data\n",
    "\n",
    "Download the latest version of iBKH knowledge graph data (entities and relations) at: https://github.com/wcm-wanglab/iBKH/tree/main/iBKH\n",
    "\n",
    "\n",
    "Please make sure putting the downloaded files following the structure below.\n",
    "\n",
    "```\n",
    ".\n",
    "├── Case Study-AD Drug Repurposing.ipynb\n",
    "├── Data\n",
    "│   ├── iBKH                                 \n",
    "│   │   ├── Entity\n",
    "│   │   │   ├── anatomy_vocab.csv\n",
    "│   │   │   ├── disease_vocab.csv\n",
    "│   │   │   ├── drug_vocab.csv\n",
    "│   │   │   ├── dsp_vocab.csv\n",
    "│   │   │   ├── gene_vocab.csv\n",
    "│   │   │   ├── molecule_vocab.csv\n",
    "│   │   │   ├── pathway_vocab.csv\n",
    "│   │   │   ├── sdsi_vocab.csv\n",
    "│   │   │   ├── side_effect_vocab.csv\n",
    "│   │   │   ├── symptom_vocab.csv\n",
    "│   │   │   ├── tc_vocab.csv\n",
    "│   │   │   ├── ...\n",
    "│   │   │   │ \n",
    "│   │   ├── Relation\n",
    "│   │   │   ├── A_G_res.csv\n",
    "│   │   │   ├── D_D_res.csv\n",
    "│   │   │   ├── D_Di_res.csv\n",
    "│   │   │   ├── D_G_res.csv\n",
    "│   │   │   ├── D_Pwy_res.csv\n",
    "│   │   │   ├── D_SE_res.csv\n",
    "│   │   │   ├── Di_Di_res.csv\n",
    "│   │   │   ├── Di_G_res.csv\n",
    "│   │   │   ├── Di_Pwy_res.csv\n",
    "│   │   │   ├── Di_Sy_res.csv\n",
    "│   │   │   ├── DSP_SDSI_res.csv\n",
    "│   │   │   ├── G_G_res.csv\n",
    "│   │   │   ├── G_Pwy_res.csv\n",
    "│   │   │   ├── SDSI_A_res.csv\n",
    "│   │   │   ├── SDSI_D_res.csv\n",
    "│   │   │   ├── SDSI_Di_res.csv\n",
    "│   │   │   ├── SDSI_Sy.csv\n",
    "│   │   │   ├── SDSI_TC_res.csv\n",
    "│   │   │   ├── ...\n",
    "│   │   │   └──                      \n",
    "│   │   └── \n",
    "│   └── ...\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('.') # Use only with Jupyter Notebook\n",
    "\n",
    "import funcs.KG_processing as KG_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Generate Triplet Set from iBKH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A triplet, i.e., (h, r, t), is the basic unit for a knowledge graph. We generate triplet set from iBKH, which will be used for knowledge graph embedding learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_folder = 'data/iBKH/' # The folder is used to store the iBKH-KG data\n",
    "triplet_path = 'data/triplets/' # The folder is used to store processed results\n",
    "if not os.path.exists(triplet_path):\n",
    "    os.makedirs(triplet_path)   \n",
    "output_path = 'data/dataset/' # Output folder\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating triplets for different entity type pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_processing.DDi_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DG_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DPwy_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DSE_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DiDi_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DiG_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DiPwy_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DiSy_triplets(kg_folder, triplet_path)\n",
    "KG_processing.GG_triplets(kg_folder, triplet_path)\n",
    "KG_processing.GPwy_triplets(kg_folder, triplet_path)\n",
    "KG_processing.DD_triplets(kg_folder, triplet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the triplets set extracted from the relation results among the entities, then convert the triplet set from .csv format to the .tsv format based on the DGL input requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying triplet type you want to use.\n",
    "included_pair_type = ['DDi', 'DiG', 'DG', 'GG', 'DD', 'DiDi',\n",
    "                      'GPwy', 'DiPwy', 'DPwy', 'DiSy',  'DSE']\n",
    "\n",
    "# Running below script will return a csv file, which combines all triplets extracted from the above functions.\n",
    "KG_processing.generate_triplet_set(triplet_path=triplet_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets.\n",
    "# And convert data to TSV files following DGK-KE requirements.\n",
    "KG_processing.generate_DGL_data_set(triplet_path=triplet_path, \n",
    "                                    output_path=output_path, \n",
    "                                    train_val_test_ratio=[.9, .05, .05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Knowledge graph embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke the command line toolkit provided by DGL-KE to learn the embedding of entities and relations in iBKH. Here, we use four different models to learn the entity and edge representations of iBKH, namely TransE, TransR, DistMult, and ComplEx. To use other KGE model or AWS instances please refer to DGL-KE’s <a href=\"https://aws-dglke.readthedocs.io/en/latest/index.html\" target=\"_blank\">Document</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open command line (Windows OS and UNIX OS) or terminal (MAC OS) and change directory as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd [your file path]/iBKH-KD-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the knowledge graph embedding model by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGLBACKEND=pytorch \\\n",
    "dglke_train --dataset iBKH --data_path ./data/dataset \\\n",
    "            --data_files training_triplets.tsv \\\n",
    "                          validation_triplets.tsv \\\n",
    "                          testing_triplets.tsv \\\n",
    "            --format raw_udd_hrt --model_name [model name] \\\n",
    "            --batch_size [batch size] --hidden_dim [hidden dim] \\\n",
    "            --neg_sample_size [neg sample size] --gamma [gamma] \\\n",
    "            --lr [learning rate] --max_step [max step] \\\n",
    "            --log_interval [log interval] \\\n",
    "            --batch_size_eval [batch size eval] \\\n",
    "            -adv --regularization_coef [regularization coef] \\\n",
    "            --num_thread [num thread] --num_proc [num proc] \\\n",
    "            --neg_sample_size_eval [neg sample size eval] \\\n",
    "            --save_path ./data/embeddings --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running above command will train the specific knowledge graph embedding model in the training dataset and evaluate the model performance in link prediction task in the testing set. This will result in multiple metrics including: Hit@k (the average number of times the positive triplet is among the k highest ranked triplets); Mean Rank (MR, the average rank of the positive triplets); Mean Reciprocal Rank (MRR, the average reciprocal rank of the positive instances). Higher values of Hit@k and MRR and a lower value of MR indicate good performance, and vice versa.\n",
    "\n",
    "\n",
    "Of note, the user can use above command to find optimal hyperparameters of the model. For simplicity, the user can also use our suggested hyperparameters as below.\n",
    "\n",
    "```\n",
    "Arguments \t            TransE\t      TransR\t  ComplEx\t    DistMult\n",
    "--model_name\t        TransE_l2\t  TransR\t  ComplEx\t    DistMult\n",
    "--batch_size\t        1024\t      1024\t      1024\t        1024\n",
    "--batch_size_eval\t    1000\t      1000\t      1000\t        1000\n",
    "--neg_sample_size\t    256\t          256\t      256\t        256\n",
    "--neg_sample_size_eval\t1000\t      1000\t      1000\t        1000\n",
    "--hidden_dim\t        400\t          200\t      200\t        400\n",
    "--gamma\t                12.0\t      12.0\t      12.0\t        12.0\n",
    "--lr\t                0.1\t          0.005\t      0.005\t        0.005\n",
    "--max_step\t            10000\t      10000\t      10000\t        10000\n",
    "--log_interval      \t100\t          100\t      100\t        100\n",
    "--regularization_coef\t1.00E-09\t  1.00E-07\t  1.00E-07\t    1.00E-07\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining hyperparameters that can lead to desirable performance, we then re-train the model using the whole dataset by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGLBACKEND=pytorch \\\n",
    "dglke_train --dataset iBKH --data_path ./data/dataset \\\n",
    "            --data_files whole_triplets.tsv \\\n",
    "            --format raw_udd_hrt --model_name [model name] \\\n",
    "            --batch_size [batch size] --hidden_dim [hidden dim] \\\n",
    "            --neg_sample_size [neg sample size] --gamma [gamma] \\\n",
    "            --lr [learning rate] --max_step [max step] \\\n",
    "            --log_interval [log interval] \\\n",
    "            -adv --regularization_coef [regularization coef] \\\n",
    "            --num_thread [num thread] --num_proc [num proc] \\\n",
    "            --save_path ./data/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate two output files for each model: “iBKH_[model name]\\_entity.npy”, containing the low dimension embeddings of entities in iBKH and “iBKH_[model name]\\_relation.npy”, containing the low dimension embeddings of relations in iBKH. These embeddings can be used in downstream knowledge discovery tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Knowledge Discovery Based on iBKH - Hypothesis Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step conducts knowledge discovery based on iBKH. \n",
    "\n",
    "We showcases an example -- drug repurposing hypothesis generation for Parkinson's disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs.KG_link_pred import generate_hypothesis,\\\n",
    "                               generate_hypothesis_ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = [\"parkinson's disease\", \"late onset parkinson's disease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_type = [\"Treats_DDi\", \"Palliates_DDi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Drug repurposing hypothesis generation based on graph embedding using the TransE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_df = generate_hypothesis(target_entity=PD, candidate_entity_type='drug',\n",
    "                                  relation_type=r_type, embedding_folder='data/embeddings',\n",
    "                                  method='transE_l2', kg_folder = 'data/iBKH', \n",
    "                                  triplet_folder = 'data/triplets', topK=100, \n",
    "                                  save_path='output', save=True,\n",
    "                                  without_any_rel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in an output CSV file stored in the \"output\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the predicted drugs.\n",
    "\n",
    "proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an ensemble model that integrates TransE, TransR, complEx, and DistMult to generate hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_proposed_df = generate_hypothesis_ensemble_model(target_entity=PD, candidate_entity_type='drug',\n",
    "                                                          relation_type=r_type, \n",
    "                                                          embedding_folder='data/embeddings',\n",
    "                                                          kg_folder = 'data/iBKH', \n",
    "                                                          triplet_folder = 'data/triplets',\n",
    "                                                          topK=100, save_path='output', save=True, \n",
    "                                                          without_any_rel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the predicted drugs using ensemble method\n",
    "ensemble_proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Interpreting prediction results in knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we interpret predicted repurposing drug candidates using knowledge graph. We can extract intermediate entities that construct the shortest paths linking the target entity (i.e., Parkinson's disease) and the predicted drug candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To achive this goal, we first deploy the iBKH knoweldge graph using Neo4j with an AWS server. Please refer the following instruction to set up the knoweldge graph: https://docs.google.com/document/d/1cLDPLp_nVCJ5xrDlJ-B-Q3wf24tb-Dyq55nAXxaNgTM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Interpreting repurposing drug candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs.knowledge_visualization as knowledge_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predicted repurposing drug candidates to interprete\n",
    "\n",
    "drug_list = ['Glutathione', 'Clioquinol', 'Steroids', 'Taurine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_visualization.subgraph_visualization(target_type='Disease', target_list=PD,\n",
    "                                               predicted_type='Drug', predicted_list=drug_list, \n",
    "                                               neo4j_url = \"neo4j://54.210.251.104:7687\", \n",
    "                                               username = \"neo4j\", password = \"password\",\n",
    "                                               alpha=1.5, k=0.8, figsize=(15, 10), save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
